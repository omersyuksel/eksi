{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "myfile = gzip.open(r'../data/all.json.gz')\n",
    "items = json.load(myfile)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date_string</th>\n",
       "      <th>entry_no</th>\n",
       "      <th>favcount</th>\n",
       "      <th>id</th>\n",
       "      <th>page_no</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sitki siyril</td>\n",
       "      <td>27.08.2004 17:33 ~ 28.05.2014 21:16</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>5526036</td>\n",
       "      <td>0</td>\n",
       "      <td>en popülerlerinden bir sözlük celebrity’si ile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sitki siyril</td>\n",
       "      <td>27.08.2004 17:35</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>5526061</td>\n",
       "      <td>0</td>\n",
       "      <td>sözlükte çok popüler, entryleri sevilen bir üç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sitki siyril</td>\n",
       "      <td>27.08.2004 17:43</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5526178</td>\n",
       "      <td>0</td>\n",
       "      <td>geç kalmadan...(bkz: ekşi itirafçıların demek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sitki siyril</td>\n",
       "      <td>27.08.2004 18:21</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5526647</td>\n",
       "      <td>0</td>\n",
       "      <td>geçen ssg ile zirvede karşılaştım. arası soğum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sitki siyril</td>\n",
       "      <td>27.08.2004 18:27</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5526710</td>\n",
       "      <td>0</td>\n",
       "      <td>amcıkgülün itirafını okuyunca aklıma geldi. be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                          date_string  entry_no  favcount  \\\n",
       "0  sitki siyril  27.08.2004 17:33 ~ 28.05.2014 21:16         0       490   \n",
       "1  sitki siyril                     27.08.2004 17:35         1        38   \n",
       "2  sitki siyril                     27.08.2004 17:43         2         5   \n",
       "3  sitki siyril                     27.08.2004 18:21         3         6   \n",
       "4  sitki siyril                     27.08.2004 18:27         4        10   \n",
       "\n",
       "        id  page_no                                               text  \n",
       "0  5526036        0  en popülerlerinden bir sözlük celebrity’si ile...  \n",
       "1  5526061        0  sözlükte çok popüler, entryleri sevilen bir üç...  \n",
       "2  5526178        0  geç kalmadan...(bkz: ekşi itirafçıların demek ...  \n",
       "3  5526647        0  geçen ssg ile zirvede karşılaştım. arası soğum...  \n",
       "4  5526710        0  amcıkgülün itirafını okuyunca aklıma geldi. be...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "favs_table = pandas.DataFrame(items)\n",
    "\n",
    "favs_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "(Stemming and vectorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import snowballstemmer\n",
    "stemmer = snowballstemmer.stemmer('turkish')\n",
    "    \n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "#Thanks to stackoverflow\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stemWord(w) for w in analyzer(doc))\n",
    "\n",
    "cv = CountVectorizer(analyzer=stemmed_words,token_pattern=ur'((?:\\w|ö|ü|ı|ğ|ş|ç)+)', min_df=2, max_df=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_vectorized = cv.fit_transform(favs_table['text'])\n",
    "dataset = dataset_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20)\n",
    "\n",
    "dataset_lda = lda.fit_transform(dataset_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##From: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words in topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "oy pazar tepki dede it sure hah işyer cidi kis\n",
      "Topic #1:\n",
      "bir cok bu ve de iç sey be am da\n",
      "Topic #2:\n",
      "saç korkuyor hav ışık kar oh mümk diş video dene\n",
      "Topic #3:\n",
      "bkz ek eksi dusunuyor 2015 ism karma dus raz edit\n",
      "Topic #4:\n",
      "istiyor çok kendi baze olmak istemiyor iç gip olmuyor zor\n",
      "Topic #5:\n",
      "şarkı film müzik ses dan com dinliyor fena günlük ilaç\n",
      "Topic #6:\n",
      "acayip su şok aci değişiklik resmi yavru evet veriç kapatıp\n",
      "Topic #7:\n",
      "kullanma hissetmiyor yalniz evle tez kanal daim zannediyor süt hâlâ\n",
      "Topic #8:\n",
      "bir ne bu da de var ya yok ve mi\n",
      "Topic #9:\n",
      "facebook ta gizli mavi profil metro bölüm gidel you hu\n",
      "Topic #10:\n",
      "bir çok bu ve iç be am şey de da\n",
      "Topic #11:\n",
      "sözlük itiraf entry yazar başlık yaz ediyor bura yazdık ekşi\n",
      "Topic #12:\n",
      "ardı of the yağmur sahne ankar hissi dans edil kırıl\n",
      "Topic #13:\n",
      "fotoğraf yük doktor beyaz verecek top niyet düze siyah karışık\n",
      "Topic #14:\n",
      "bir ve ev gün iç geç sonra sabah saat g\n",
      "Topic #15:\n",
      "allah keşke mutsuz kedi hissettik millet mutsuzluk ver u herşey\n",
      "Topic #16:\n",
      "bişey sınıf çocuk okul gül peş tatlı bık sanıyor bilemedi\n",
      "Topic #17:\n",
      "bi ben dedi de di da am ol ne sonra\n",
      "Topic #18:\n",
      "mesaj kitap hayaller okuyor tane yemek cevap hayal den veya\n",
      "Topic #19:\n",
      "el gip ayak yer yatak dip çekiyor soğuk ve depresyo\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, cv.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and test sets, ordered by time of entry. We are trying to predict the popularity of the `future' entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = favs_table['favcount'].as_matrix()\n",
    "\n",
    "dataset = dataset_lda\n",
    "train_index = int(0.8*len(dataset))\n",
    "training_X = dataset[0:train_index]\n",
    "training_y = y[0:train_index]\n",
    "test_X = dataset[train_index:]\n",
    "test_y = y[train_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 classes: 0 favs, 1-4 favs (somewhat popular), popular. I admit that the distinction is a bit arbitrary, but wanted to avoid a regression problem here (as I don't really care whether an entry gets 50 favs or 1000). An alternative is to split them into classes by quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discretize_y(in_y):\n",
    "    ret = []\n",
    "    for yi in in_y:\n",
    "        if yi in [0]:\n",
    "            ret.append(yi)\n",
    "        elif yi < 5:\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(2)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_y_discrete = discretize_y(training_y)\n",
    "test_y_discrete = discretize_y(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=4, max_features=None)#, class_weight={0:3,1:1,2:50})\n",
    "clf.fit(training_X, training_y_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on training data:\n",
      "Accuracy:\n",
      "0.987002810203\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[17152,   830,     0],\n",
       "       [   75, 54278,     2],\n",
       "       [    0,    55,  1624]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Results on training data:'\n",
    "\n",
    "results_y = clf.predict(training_X[:])\n",
    "\n",
    "print 'Accuracy:'\n",
    "print accuracy_score(training_y_discrete[:], results_y)\n",
    "\n",
    "print 'Confusion matrix:'\n",
    "confusion_matrix(training_y_discrete[:], results_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy (with shuffling):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.72257498,  0.72149419,  0.72194825,  0.72552861,  0.72199703])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "print 'Cross validation accuracy (with shuffling):'\n",
    "\n",
    "cross_validation.cross_val_score(clf, training_X, training_y_discrete, \n",
    "                                 cv=cross_validation.StratifiedKFold(training_y_discrete, n_folds=5, shuffle=True ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On test data:\n",
      "Accuracy:\n",
      "0.51934716818\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 171, 6254,    4],\n",
       "       [ 285, 9431,    4],\n",
       "       [  41, 2306,    8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'On test data:'\n",
    "results_y = clf.predict(test_X)\n",
    "\n",
    "print 'Accuracy:'\n",
    "print accuracy_score(test_y_discrete, results_y)\n",
    "\n",
    "print 'Confusion matrix:'\n",
    "confusion_matrix(test_y_discrete, results_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Naive Bayes\n",
    "(Simpler model compared to random forests, should be less prone to overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(training_X, training_y_discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on training data:\n",
      "Accuracy:\n",
      "0.645319930826\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1382, 14983,  1617],\n",
       "       [ 3355, 45963,  5037],\n",
       "       [  133,  1127,   419]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Results on training data:'\n",
    "\n",
    "results_y = clf.predict(training_X[:])\n",
    "\n",
    "print 'Accuracy:'\n",
    "print accuracy_score(training_y_discrete[:], results_y)\n",
    "\n",
    "print 'Confusion matrix:'\n",
    "confusion_matrix(training_y_discrete[:], results_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy (with shuffling):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.64462307,  0.64516347,  0.66601365,  0.63352023,  0.64004864])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "print 'Cross validation accuracy (with shuffling):'\n",
    "\n",
    "cross_validation.cross_val_score(clf, training_X, training_y_discrete, \n",
    "                                 cv=cross_validation.StratifiedKFold(training_y_discrete, n_folds=5, shuffle=True ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46687202767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 535, 5340,  554],\n",
       "       [ 739, 7556, 1425],\n",
       "       [ 177, 1630,  548]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_y = clf.predict(test_X)\n",
    "\n",
    "print accuracy_score(test_y_discrete, results_y)\n",
    "\n",
    "confusion_matrix(test_y_discrete, results_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary of the results*: Vectorized text alone does not help predicting popularity. \n",
    "\n",
    "Tried different feature spaces, such as using tf-idf+bag-of-words instead of topics, or using other dimensionality reduction methods such as LSA. Also tried other models (SVM, Logistic regression). They do not provide any significant improvement over the current results.\n",
    "\n",
    "*Things to try:* Combine text-related features with others: time of the day, entry length, perhaps the author or metadata related to author. Phrase detection to get better features than singular words.\n",
    "\n",
    "Also try to use unsupervised methods on popular entries to see what features they have in common.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
